{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the MNIST handwritten numbers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"C:/Users/lucif/Documents/Jupiter_Notebook/Datasets/mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 3, 6, 7, 8], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating and One Hot Encoding the labels from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = training_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.eye(10,10)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.drop(['label'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>1x10</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  1x10  ...  28x19  28x20  \\\n",
       "0    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "1    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "2    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "3    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "4    0    0    0    0    0    0    0    0    0     0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Funtion to return Posterior Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_probabilities(t0s,ts,X):\n",
    "    \n",
    "    likelihood_probabilities = np.exp(np.matmul(X,ts) + t0s)\n",
    "    \n",
    "    #print(likelihood_probabilities.shape)\n",
    "    \n",
    "    normalizing_probabilities = np.sum(likelihood_probabilities,axis=1)\n",
    "    \n",
    "    normalizing_probabilities = normalizing_probabilities.reshape(normalizing_probabilities.shape[0],1)\n",
    "    \n",
    "    #print(normalizing_probabilities.shape)\n",
    "    \n",
    "    posterior_probabilities = (likelihood_probabilities/normalizing_probabilities)\n",
    "    \n",
    "    return posterior_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Funtions to return derivatives with respect to wieghts for GDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delbydel0s(S,C):\n",
    "    \n",
    "    return np.sum((S-C),axis=0)/C.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delbydels(S,C,X):\n",
    "    \n",
    "    return np.matmul(X.T,(S-C))/C.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funtion to calculate Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_with_logits(S,C):\n",
    "    \n",
    "    return -np.sum((np.log(S)*C))/C.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing variables for GDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 10**(-4)\n",
    "\n",
    "learning_rate = 10**(-5)\n",
    "\n",
    "iterations = []\n",
    "\n",
    "neg_log_loss_history = []\n",
    "\n",
    "t0s_initial = np.zeros((1,10))\n",
    "\n",
    "ts_initial = np.zeros((784,10))\n",
    "\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration = 0 loss = 2.3025850929940437\n",
      "iteration = 1 loss = 1.719786263215253\n",
      "iteration = 2 loss = 1.3874725510775354\n",
      "iteration = 3 loss = 1.1929573266014326\n",
      "iteration = 4 loss = 1.0867972114481212\n",
      "iteration = 5 loss = 0.9944650866172627\n",
      "iteration = 6 loss = 0.9694866871346774\n",
      "iteration = 7 loss = 0.8473392642491232\n",
      "iteration = 8 loss = 0.8233635174873839\n",
      "iteration = 9 loss = 0.7672825655682719\n",
      "iteration = 10 loss = 0.7526967207105258\n",
      "iteration = 11 loss = 0.703304138996503\n",
      "iteration = 12 loss = 0.6863643413837363\n",
      "iteration = 13 loss = 0.6537223648291108\n",
      "iteration = 14 loss = 0.6370964275517209\n",
      "iteration = 15 loss = 0.6150473926755196\n",
      "iteration = 16 loss = 0.600478014860785\n",
      "iteration = 17 loss = 0.5851417350701887\n",
      "iteration = 18 loss = 0.5731908377567501\n",
      "iteration = 19 loss = 0.5619329130506622\n",
      "iteration = 20 loss = 0.552313271160402\n",
      "iteration = 21 loss = 0.5435487619689013\n",
      "iteration = 22 loss = 0.5357035496414622\n",
      "iteration = 23 loss = 0.5285320193412899\n",
      "iteration = 24 loss = 0.5219516104981374\n",
      "iteration = 25 loss = 0.5158575086218841\n",
      "iteration = 26 loss = 0.5101805656472497\n",
      "iteration = 27 loss = 0.5048637642258484\n",
      "iteration = 28 loss = 0.49986360783889866\n",
      "iteration = 29 loss = 0.49514552166890946\n",
      "iteration = 30 loss = 0.49068161853127334\n",
      "iteration = 31 loss = 0.4864487059490616\n",
      "iteration = 32 loss = 0.4824270717717838\n",
      "iteration = 33 loss = 0.47859958156063526\n",
      "iteration = 34 loss = 0.4749511462255065\n",
      "iteration = 35 loss = 0.47146832687506046\n",
      "iteration = 36 loss = 0.46813907941088817\n",
      "iteration = 37 loss = 0.4649525519273553\n",
      "iteration = 38 loss = 0.46189893238305396\n",
      "iteration = 39 loss = 0.458969320317799\n",
      "iteration = 40 loss = 0.45615562085755956\n",
      "iteration = 41 loss = 0.45345045323550676\n",
      "iteration = 42 loss = 0.4508470723262821\n",
      "iteration = 43 loss = 0.4483393003188194\n",
      "iteration = 44 loss = 0.44592146723894865\n",
      "iteration = 45 loss = 0.4435883588510673\n",
      "iteration = 46 loss = 0.441335170937729\n",
      "iteration = 47 loss = 0.43915746902836733\n",
      "iteration = 48 loss = 0.4370511528399878\n",
      "iteration = 49 loss = 0.4350124247886613\n",
      "iteration = 50 loss = 0.4330377620364291\n",
      "iteration = 51 loss = 0.43112389161456804\n",
      "iteration = 52 loss = 0.4292677682327301\n",
      "iteration = 53 loss = 0.42746655443887527\n",
      "iteration = 54 loss = 0.42571760284234117\n",
      "iteration = 55 loss = 0.42401844015205253\n",
      "iteration = 56 loss = 0.42236675281559927\n",
      "iteration = 57 loss = 0.4207603740734771\n",
      "iteration = 58 loss = 0.4191972722671489\n",
      "iteration = 59 loss = 0.41767554026038833\n",
      "iteration = 60 loss = 0.4161933858511783\n",
      "iteration = 61 loss = 0.4147491230667622\n",
      "iteration = 62 loss = 0.4133411642476081\n",
      "iteration = 63 loss = 0.41196801283744844\n",
      "iteration = 64 loss = 0.410628256806399\n",
      "iteration = 65 loss = 0.4093205626427119\n",
      "iteration = 66 loss = 0.40804366985615265\n",
      "iteration = 67 loss = 0.4067963859424659\n",
      "iteration = 68 loss = 0.40557758176405817\n",
      "iteration = 69 loss = 0.40438618730697107\n",
      "iteration = 70 loss = 0.4032211877785683\n",
      "iteration = 71 loss = 0.4020816200141716\n",
      "iteration = 72 loss = 0.4009665691642404\n",
      "iteration = 73 loss = 0.3998751656366623\n",
      "iteration = 74 loss = 0.39880658227132043\n",
      "iteration = 75 loss = 0.3977600317264483\n",
      "iteration = 76 loss = 0.39673476405831276\n",
      "iteration = 77 loss = 0.39573006447760556\n",
      "iteration = 78 loss = 0.3947452512675483\n",
      "iteration = 79 loss = 0.39377967385014806\n",
      "iteration = 80 loss = 0.39283271098835054\n",
      "iteration = 81 loss = 0.39190376911297653\n",
      "iteration = 82 loss = 0.39099228076436693\n",
      "iteration = 83 loss = 0.3900977031395849\n",
      "iteration = 84 loss = 0.3892195167368534\n",
      "iteration = 85 loss = 0.38835722408965073\n",
      "iteration = 86 loss = 0.38751034858356576\n",
      "iteration = 87 loss = 0.3866784333496071\n",
      "iteration = 88 loss = 0.3858610402282145\n",
      "iteration = 89 loss = 0.38505774879871546\n",
      "iteration = 90 loss = 0.38426815546940324\n",
      "iteration = 91 loss = 0.38349187262383316\n",
      "iteration = 92 loss = 0.3827285278192838\n",
      "iteration = 93 loss = 0.38197776303367037\n",
      "iteration = 94 loss = 0.38123923395749687\n",
      "iteration = 95 loss = 0.3805126093277074\n",
      "iteration = 96 loss = 0.3797975703005426\n",
      "iteration = 97 loss = 0.37909380986074326\n",
      "iteration = 98 loss = 0.3784010322646413\n",
      "iteration = 99 loss = 0.37771895251486937\n",
      "iteration = 100 loss = 0.3770472958645998\n",
      "iteration = 101 loss = 0.37638579734937194\n",
      "iteration = 102 loss = 0.3757342013447191\n",
      "iteration = 103 loss = 0.3750922611479336\n",
      "iteration = 104 loss = 0.3744597385824368\n",
      "iteration = 105 loss = 0.3738364036233221\n",
      "iteration = 106 loss = 0.3732220340427552\n",
      "iteration = 107 loss = 0.3726164150739949\n",
      "iteration = 108 loss = 0.3720193390928997\n",
      "iteration = 109 loss = 0.3714306053158514\n",
      "iteration = 110 loss = 0.3708500195131116\n",
      "iteration = 111 loss = 0.37027739373668817\n",
      "iteration = 112 loss = 0.3697125460618541\n",
      "iteration = 113 loss = 0.36915530034151967\n",
      "iteration = 114 loss = 0.3686054859727076\n",
      "iteration = 115 loss = 0.3680629376744392\n",
      "iteration = 116 loss = 0.3675274952763769\n",
      "iteration = 117 loss = 0.36699900351761294\n",
      "iteration = 118 loss = 0.3664773118550401\n",
      "iteration = 119 loss = 0.36596227428076483\n",
      "iteration = 120 loss = 0.3654537491480679\n",
      "iteration = 121 loss = 0.36495159900544405\n",
      "iteration = 122 loss = 0.36445569043828074\n",
      "iteration = 123 loss = 0.3639658939177658\n",
      "iteration = 124 loss = 0.36348208365663887\n",
      "iteration = 125 loss = 0.36300413747142035\n",
      "iteration = 126 loss = 0.36253193665078354\n",
      "iteration = 127 loss = 0.3620653658297437\n",
      "iteration = 128 loss = 0.36160431286936456\n",
      "iteration = 129 loss = 0.3611486687417039\n",
      "iteration = 130 loss = 0.3606983274197249\n",
      "iteration = 131 loss = 0.3602531857719242\n",
      "iteration = 132 loss = 0.3598131434614433\n",
      "iteration = 133 loss = 0.35937810284943567\n",
      "iteration = 134 loss = 0.35894796890248176\n",
      "iteration = 135 loss = 0.35852264910385406\n",
      "iteration = 136 loss = 0.3581020533684417\n",
      "iteration = 137 loss = 0.35768609396116086\n",
      "iteration = 138 loss = 0.35727468541868324\n",
      "iteration = 139 loss = 0.3568677444743218\n",
      "iteration = 140 loss = 0.3564651899859296\n",
      "iteration = 141 loss = 0.3560669428666649\n",
      "iteration = 142 loss = 0.3556729260184918\n",
      "iteration = 143 loss = 0.3552830642682916\n",
      "iteration = 144 loss = 0.3548972843064581\n",
      "iteration = 145 loss = 0.35451551462787395\n",
      "iteration = 146 loss = 0.3541376854751486\n",
      "iteration = 147 loss = 0.35376372878402507\n",
      "iteration = 148 loss = 0.3533935781308549\n",
      "iteration = 149 loss = 0.3530271686820467\n",
      "iteration = 150 loss = 0.3526644371454085\n",
      "iteration = 151 loss = 0.3523053217232941\n",
      "iteration = 152 loss = 0.351949762067477\n",
      "iteration = 153 loss = 0.3515976992356786\n",
      "iteration = 154 loss = 0.35124907564967933\n",
      "iteration = 155 loss = 0.3509038350549424\n",
      "iteration = 156 loss = 0.35056192248169027\n",
      "iteration = 157 loss = 0.350223284207368\n",
      "iteration = 158 loss = 0.3498878677204404\n",
      "iteration = 159 loss = 0.34955562168546334\n",
      "iteration = 160 loss = 0.3492264959093779\n",
      "iteration = 161 loss = 0.34890044130897885\n",
      "iteration = 162 loss = 0.3485774098795063\n",
      "iteration = 163 loss = 0.3482573546643185\n",
      "iteration = 164 loss = 0.3479402297255982\n",
      "iteration = 165 loss = 0.3476259901160556\n",
      "iteration = 166 loss = 0.3473145918515847\n",
      "iteration = 167 loss = 0.3470059918848354\n",
      "iteration = 168 loss = 0.3467001480796698\n",
      "iteration = 169 loss = 0.346397019186461\n",
      "iteration = 170 loss = 0.34609656481820816\n",
      "iteration = 171 loss = 0.345798745427431\n",
      "iteration = 172 loss = 0.3455035222838193\n",
      "iteration = 173 loss = 0.34521085745260244\n",
      "iteration = 174 loss = 0.34492071377361727\n",
      "iteration = 175 loss = 0.34463305484104334\n",
      "iteration = 176 loss = 0.34434784498378374\n",
      "iteration = 177 loss = 0.344065049246466\n",
      "iteration = 178 loss = 0.34378463337104\n",
      "iteration = 179 loss = 0.34350656377895006\n",
      "iteration = 180 loss = 0.3432308075538641\n",
      "iteration = 181 loss = 0.3429573324249332\n",
      "iteration = 182 loss = 0.3426861067505666\n",
      "iteration = 183 loss = 0.34241709950270444\n",
      "iteration = 184 loss = 0.3421502802515672\n",
      "iteration = 185 loss = 0.3418856191508641\n",
      "iteration = 186 loss = 0.3416230869234506\n",
      "iteration = 187 loss = 0.3413626548474114\n",
      "iteration = 188 loss = 0.34110429474255793\n",
      "iteration = 189 loss = 0.3408479789573259\n",
      "iteration = 190 loss = 0.3405936803560589\n",
      "iteration = 191 loss = 0.34034137230666217\n",
      "iteration = 192 loss = 0.3400910286686175\n",
      "iteration = 193 loss = 0.33984262378134533\n",
      "iteration = 194 loss = 0.33959613245290016\n",
      "iteration = 195 loss = 0.33935152994899276\n",
      "iteration = 196 loss = 0.33910879198232274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration = 197 loss = 0.33886789470221484\n",
      "iteration = 198 loss = 0.33862881468454903\n",
      "iteration = 199 loss = 0.33839152892196994\n",
      "iteration = 200 loss = 0.33815601481437285\n",
      "iteration = 201 loss = 0.33792225015965344\n",
      "iteration = 202 loss = 0.33769021314471137\n",
      "iteration = 203 loss = 0.3374598823367042\n",
      "iteration = 204 loss = 0.33723123667453875\n",
      "iteration = 205 loss = 0.33700425546059515\n",
      "iteration = 206 loss = 0.33677891835267515\n",
      "iteration = 207 loss = 0.33655520535616706\n",
      "iteration = 208 loss = 0.33633309681642093\n",
      "iteration = 209 loss = 0.33611257341133\n",
      "iteration = 210 loss = 0.335893616144104\n",
      "iteration = 211 loss = 0.3356762063362404\n",
      "iteration = 212 loss = 0.33546032562067407\n",
      "iteration = 213 loss = 0.33524595593511153\n",
      "iteration = 214 loss = 0.33503307951553646\n",
      "iteration = 215 loss = 0.33482167888988423\n",
      "iteration = 216 loss = 0.3346117368718804\n",
      "iteration = 217 loss = 0.33440323655503706\n",
      "iteration = 218 loss = 0.334196161306803\n",
      "iteration = 219 loss = 0.33399049476286313\n",
      "iteration = 220 loss = 0.3337862208215819\n",
      "iteration = 221 loss = 0.33358332363858806\n",
      "iteration = 222 loss = 0.33338178762149384\n",
      "iteration = 223 loss = 0.3331815974247476\n",
      "iteration = 224 loss = 0.3329827379446152\n",
      "iteration = 225 loss = 0.33278519431428194\n",
      "iteration = 226 loss = 0.33258895189908183\n",
      "iteration = 227 loss = 0.33239399629183747\n",
      "iteration = 228 loss = 0.33220031330831834\n",
      "iteration = 229 loss = 0.3320078889828075\n",
      "iteration = 230 loss = 0.3318167095637767\n",
      "iteration = 231 loss = 0.3316267615096649\n",
      "iteration = 232 loss = 0.33143803148476053\n",
      "iteration = 233 loss = 0.33125050635517833\n",
      "iteration = 234 loss = 0.33106417318493586\n",
      "iteration = 235 loss = 0.3308790192321216\n",
      "iteration = 236 loss = 0.3306950319451518\n",
      "iteration = 237 loss = 0.3305121989591187\n",
      "iteration = 238 loss = 0.33033050809222203\n",
      "iteration = 239 loss = 0.3301499473422835\n",
      "iteration = 240 loss = 0.3299705048833433\n",
      "iteration = 241 loss = 0.3297921690623353\n",
      "iteration = 242 loss = 0.32961492839583834\n",
      "iteration = 243 loss = 0.32943877156690055\n",
      "iteration = 244 loss = 0.3292636874219391\n",
      "iteration = 245 loss = 0.329089664967709\n",
      "iteration = 246 loss = 0.3289166933683396\n",
      "iteration = 247 loss = 0.3287447619424388\n",
      "iteration = 248 loss = 0.32857386016026296\n",
      "iteration = 249 loss = 0.32840397764094936\n",
      "iteration = 250 loss = 0.3282351041498086\n",
      "iteration = 251 loss = 0.3280672295956816\n",
      "iteration = 252 loss = 0.32790034402834994\n",
      "iteration = 253 loss = 0.327734437636006\n",
      "iteration = 254 loss = 0.32756950074277863\n",
      "iteration = 255 loss = 0.3274055238063108\n",
      "iteration = 256 loss = 0.32724249741539235\n",
      "iteration = 257 loss = 0.327080412287643\n",
      "iteration = 258 loss = 0.3269192592672439\n",
      "iteration = 259 loss = 0.32675902932272094\n",
      "iteration = 260 loss = 0.3265997135447724\n",
      "iteration = 261 loss = 0.32644130314414543\n",
      "iteration = 262 loss = 0.32628378944955544\n",
      "iteration = 263 loss = 0.32612716390565183\n",
      "iteration = 264 loss = 0.32597141807102364\n",
      "iteration = 265 loss = 0.32581654361625045\n",
      "iteration = 266 loss = 0.32566253232199177\n",
      "iteration = 267 loss = 0.3255093760771154\n",
      "iteration = 268 loss = 0.32535706687686866\n",
      "iteration = 269 loss = 0.3252055968210824\n",
      "iteration = 270 loss = 0.32505495811241497\n",
      "iteration = 271 loss = 0.32490514305463297\n",
      "iteration = 272 loss = 0.32475614405092373\n",
      "iteration = 273 loss = 0.32460795360224576\n",
      "iteration = 274 loss = 0.32446056430571063\n",
      "iteration = 275 loss = 0.3243139688529973\n",
      "iteration = 276 loss = 0.32416816002880033\n",
      "iteration = 277 loss = 0.32402313070930727\n",
      "iteration = 278 loss = 0.32387887386070763\n",
      "iteration = 279 loss = 0.3237353825377303\n",
      "iteration = 280 loss = 0.32359264988221276\n",
      "iteration = 281 loss = 0.3234506691216942\n",
      "iteration = 282 loss = 0.3233094335680403\n",
      "iteration = 283 loss = 0.32316893661609364\n",
      "iteration = 284 loss = 0.32302917174235\n",
      "iteration = 285 loss = 0.3228901325036604\n",
      "iteration = 286 loss = 0.3227518125359594\n",
      "iteration = 287 loss = 0.32261420555301745\n",
      "iteration = 288 loss = 0.3224773053452173\n",
      "iteration = 289 loss = 0.32234110577835323\n",
      "iteration = 290 loss = 0.3222056007924544\n",
      "iteration = 291 loss = 0.32207078440063075\n",
      "iteration = 292 loss = 0.3219366506879391\n",
      "iteration = 293 loss = 0.321803193810272\n",
      "iteration = 294 loss = 0.3216704079932692\n",
      "iteration = 295 loss = 0.32153828753124525\n",
      "iteration = 296 loss = 0.3214068267861422\n",
      "iteration = 297 loss = 0.32127602018649765\n",
      "iteration = 298 loss = 0.3211458622264351\n",
      "iteration = 299 loss = 0.32101634746467117\n",
      "iteration = 300 loss = 0.32088747052354166\n",
      "iteration = 301 loss = 0.32075922608804697\n",
      "iteration = 302 loss = 0.3206316089049117\n",
      "iteration = 303 loss = 0.32050461378166517\n",
      "iteration = 304 loss = 0.32037823558573697\n",
      "iteration = 305 loss = 0.3202524692435683\n",
      "iteration = 306 loss = 0.3201273097397408\n",
      "iteration = 307 loss = 0.32000275211612095\n",
      "iteration = 308 loss = 0.31987879147101783\n",
      "iteration = 309 loss = 0.3197554229583598\n",
      "iteration = 310 loss = 0.3196326417868816\n",
      "iteration = 311 loss = 0.3195104432193305\n",
      "iteration = 312 loss = 0.3193888225716814\n",
      "iteration = 313 loss = 0.31926777521237026\n",
      "iteration = 314 loss = 0.31914729656153956\n",
      "iteration = 315 loss = 0.31902738209029535\n",
      "iteration = 316 loss = 0.31890802731998047\n",
      "iteration = 317 loss = 0.3187892278214576\n",
      "iteration = 318 loss = 0.3186709792144067\n",
      "iteration = 319 loss = 0.3185532771666338\n",
      "iteration = 320 loss = 0.31843611739339156\n",
      "iteration = 321 loss = 0.31831949565671264\n",
      "iteration = 322 loss = 0.3182034077647532\n",
      "iteration = 323 loss = 0.31808784957114866\n",
      "iteration = 324 loss = 0.3179728169743797\n",
      "iteration = 325 loss = 0.3178583059171501\n",
      "iteration = 326 loss = 0.3177443123857739\n",
      "iteration = 327 loss = 0.31763083240957346\n",
      "iteration = 328 loss = 0.317517862060289\n",
      "iteration = 329 loss = 0.31740539745149526\n",
      "iteration = 330 loss = 0.3172934347380302\n",
      "iteration = 331 loss = 0.31718197011543253\n",
      "iteration = 332 loss = 0.31707099981938924\n",
      "iteration = 333 loss = 0.31696052012519094\n",
      "iteration = 334 loss = 0.31685052734719704\n",
      "iteration = 335 loss = 0.31674101783831066\n",
      "iteration = 336 loss = 0.3166319879894602\n",
      "iteration = 337 loss = 0.31652343422909135\n",
      "iteration = 338 loss = 0.31641535302266577\n",
      "iteration = 339 loss = 0.31630774087216956\n",
      "iteration = 340 loss = 0.3162005943156287\n",
      "iteration = 341 loss = 0.3160939099266319\n",
      "iteration = 342 loss = 0.31598768431386276\n",
      "iteration = 343 loss = 0.3158819141206384\n",
      "iteration = 344 loss = 0.3157765960244546\n",
      "iteration = 345 loss = 0.31567172673654087\n",
      "iteration = 346 loss = 0.3155673030014202\n",
      "iteration = 347 loss = 0.3154633215964767\n",
      "iteration = 348 loss = 0.3153597793315297\n",
      "iteration = 349 loss = 0.315256673048416\n",
      "iteration = 350 loss = 0.31515399962057733\n",
      "iteration = 351 loss = 0.3150517559526536\n",
      "iteration = 352 loss = 0.3149499389800856\n",
      "iteration = 353 loss = 0.31484854566872\n",
      "iteration = 354 loss = 0.31474757301442363\n",
      "iteration = 355 loss = 0.31464701804270195\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    \n",
    "    logits_initial = softmax_probabilities(t0s_initial,ts_initial,training_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    t0s_final = t0s_initial - (learning_rate * delbydel0s(logits_initial,C))\n",
    "    \n",
    "    ts_final = ts_initial - (learning_rate * delbydels(logits_initial,C,training_data))\n",
    "    \n",
    "    \n",
    "    \n",
    "    logits_final = softmax_probabilities(t0s_final,ts_final,training_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if abs(cross_entropy_loss_with_logits(logits_initial,C)-cross_entropy_loss_with_logits(logits_final,C)) < epsilon:\n",
    "        \n",
    "        break \n",
    "        \n",
    "    \n",
    "    iterations.append(i)\n",
    "    \n",
    "    neg_log_loss_history.append(cross_entropy_loss_with_logits(logits_initial,C))\n",
    "    \n",
    "    t0s_initial = t0s_final\n",
    "    \n",
    "    ts_initial = ts_final\n",
    "    \n",
    "    print(\"iteration =\",i,\"loss =\",neg_log_loss_history[i])\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the testing data and processing the same way we did with Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = pd.read_csv(\"C:/Users/lucif/Documents/Jupiter_Notebook/Datasets/mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_labels = testing_data['label']\n",
    "\n",
    "testing_data.drop(['label'],axis=1,inplace=True)\n",
    "\n",
    "testing_data = np.array(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting result on both the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes_test = np.argmax(softmax_probabilities(t0s_final,ts_final,testing_data),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_test = (np.count_nonzero(np.equal(predicted_classes_test,testing_labels))/10000)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testing data = 91.61\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on testing data =',accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes_train = np.argmax(softmax_probabilities(t0s_final,ts_final,training_data),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = (np.count_nonzero(np.equal(predicted_classes_train,labels))/60000)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data = 91.25666666666666\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on training data =',accuracy_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Plotting Curves to Visualize the fit of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x206ce834e10>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.plot(iterations,neg_log_loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
